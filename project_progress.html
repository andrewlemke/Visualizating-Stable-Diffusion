<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project Progress: Milestone 2</title>
    <!--  Bootstrap library-->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.0/dist/css/bootstrap.min.css"
          rel="stylesheet"/>
    <!--  Font awesome-->
    <script src="https://kit.fontawesome.com/cc0f635f1e.js" crossorigin="anonymous"></script>
    <!--  Style sheet link-->
    <link href="/style.css" rel="stylesheet" />

</head>

<body class = "bg-light text-dark">

<!--Main container with margin on top and bottom-->
<div class="container p-2 my-3">
    <h2 class="fw-bold text-primary">Project Progress: Milestone 2</h2>
    <h3 class = "mt-5 text-nowrap fw-bold">Deep Learning Project Progress Diffusion Image Generation
        Visualization</h3>
    <h5 class = "fst-italic text-muted mb-1">- Andrew Lemke & Lakshyana KC</h5>
    <!--  Section start-->
    <div class="col-xxl-15 col-xl-10 col-lg-8 col-md-11 col-sm-10 col-15">

        <div class="mx-2">

            <!-- Experiment 1-->
            <div class = "text-dark" >
                <h4 class="mt-5 fw-bolder text-nowrap">Experiment 1: Visualization of
                    Intermediate Denoised Images</h4>

                <p class="fulljustify"> The standard visualization for diffusion models is
                    capturing the image at
                several points along the diffusion. The visualization appears as noise being
                removed from some image. While this visualization is nice to look at, it doesn’t clue
                in to the “magic” happening. The viewers aren’t informed as to how or why the
                noise is reduced, or how a coherent image is formed from pure noise.
                One strategy our group tried aims to discover what is present in the noise and
                model by quickly denoising the image at those intermediate positions. This rapid
                de-noising attempts to show what information is held in the model and visible
                noisy image.
                </p>

                <h5 class="fw-bolder text-nowrap">Creating the New Visualization</h5>

                <p class="fulljustify"> For this visualization, we started with diffusion code from
                    a hugging face blog post. The code covers setting up and training the model.
                    The code outputs the standard visualization as a gif. This blog post uses the
                    MNIST fashion dataset to train the model on as its small size allows for the
                    model to train on systems with normal performance (taking overnight on Google
                    Colab’s GPU runtime).
                    To achieve our ends, we had to rewrite the sampler code to allow more freedom
                    when performing diffusion. The sampling code is what uses the model to create
                    an image from noise. Our new sampler allowed us to totally customize the
                    schedule, which is the constant that affects the rate of diffusion. To see
                    the information held in the model and weights at various points in the diffusion,
                    we used large betas to rapidly denoise the image in a number of steps.
                    To remain fair, we kept the total sum of the betas consistent. Below is
                    the beta schedules for various “jumps,” which are the points where we
                    dramatically denoised the image very quickly.
                </p>

                <!--        Figure -->
                <div class="position-relative">
                    <img src="./img/fig1.jpeg" class="img-fluid w-90 wd-border-y" />
                    <span class="text-white fs-4 fw-bold position-relative p-2"></span>
                </div>

                <p class="fulljustify">
                    Here are the cumulative sums of the betas for each of the jumps.
                </p>

                <!--        Figure -->
                <div class="position-relative">
                    <img src="./img/fig2.jpeg" class="img-fluid w-70 wd-border-y" />
                    <span class="text-white fs-4 fw-bold position-relative p-2"></span>
                </div>

                <p class="fulljustify">
                    These betas produced the following visualizations from the noisy images.
                    To compare this method to the traditional visualization, it is shown below the
                    new visualization for the same seed and model randomness.
                </p>

                <!--        Figure -->
                <div class="position-relative">
                    <img src="./img/mnist2.jpeg" class="img-fluid w-70 wd-border-y" />
                    <span class="text-white fs-4 fw-bold position-relative p-2"></span>
                </div>

                <!--        Figure -->
                <div class="position-relative">
                    <img src="./img/minist1.jpeg" class="img-fluid w-70 wd-border-y" />
                    <span class="text-white fs-4 fw-bold position-relative p-2"></span>
                </div>


                <p class="fulljustify">
                    The new visualization shows some features of the output much quicker than the
                    traditional visualization. All of the MNIST fashion pictures which the model
                    was trained on are on a black border. The new method has this black border more
                    prominent than the new visualization. Unfortunately, the new method does not
                    show any large improvement over the traditional method. This could be due to
                    the ultra-simplistic diffusion model created or to flaws in this approach.
                </p>

            </div>

            <!-- Experiment 2-->
            <div class = "text-dark" >
                <h4 class="mt-5 fw-bolder text-wrap">Experiment 2: Probing into Stable Diffusion
                    Model's Understanding from Prompt and Model Output Comparision
                </h4>

                <p class="fulljustify"> Stable Diffusion Models represent a major milestone in
                    the progress of text-to-image generation task, with its hyper-realistic
                    rendering and image generation capabilities. Its outputs suggest the model's
                    ability to compose ideas and concepts that cannot simply be picked up from
                    some of the images in the training data distribution. But what is the scope of
                    this model and in what ways can we probe into the image outputs themselves to
                    get a better understanding of the model's performance and limitations from the
                    point of view of understanding of tangible concepts and ideas such common verbs,
                    nouns, quantity and so on.
                </p>

                <h5 class="fw-bolder text-nowrap">Inference Outputs</h5>

                <p class="fulljustify">
                    Some of the model outputs from the initial experimentation are shown below.
                </p>

                <!--        Figure -->
                <figure class="figure">
                    <img src="./img/sd_monkey.png"
                         class="figure-img img-fluid w-90 wd-border-y">
                    <figcaption class="figure-caption fst-italic text-center">Figure: Image
                        Outputs for
                        Prompt:
                        Monkey with a
                        Halloween Costume. Inference Steps: 50</figcaption>
                </figure>

                <!--        Figure -->
                <figure class="figure">
                    <img src="./img/sd_cat.png" class="figure-img img-fluid w-90 wd-border-y center">
                    <figcaption class="figure-caption fst-italic text-center">Figure: Image
                        Outputs for
                        Prompt:
                        Cat chewing a gum smiling</figcaption>
                </figure>

                <p class="">
                    Some of the model outputs from the initial experimentation are shown below.
                </p>

                <!--        Figure -->
                <div class="position-relative">
                    <img src="./img/fig1.jpeg" class="img-fluid w-90 wd-border-y" />
                    <span class="text-white fs-4 fw-bold position-relative p-2"></span>
                </div>

                <p class="fulljustify">
                    Here are the cumulative sums of the betas for each of the jumps.
                </p>


                <p class="fulljustify">
                    These betas produced the following visualizations from the noisy images.
                    To compare this method to the traditional visualization, it is shown below the
                    new visualization for the same seed and model randomness.
                </p>

                <!--        Figure -->
                <div class="position-relative">
                    <img src="./img/mnist2.jpeg" class="img-fluid w-70 wd-border-y" />
                    <span class="text-white fs-4 fw-bold position-relative p-2"></span>
                </div>

                <!--        Figure -->
                <div class="position-relative">
                    <img src="./img/minist1.jpeg" class="img-fluid w-70 wd-border-y" />
                    <span class="text-white fs-4 fw-bold position-relative p-2"></span>
                </div>


                <p class="fulljustify">
                    The new visualization shows some features of the output much quicker than the
                    traditional visualization. All of the MNIST fashion pictures which the model
                    was trained on are on a black border. The new method has this black border more
                    prominent than the new visualization. Unfortunately, the new method does not
                    show any large improvement over the traditional method. This could be due to
                    the ultra-simplistic diffusion model created or to flaws in this approach.
                </p>

                <!--                    and-->
                <!--                    Attention Heat Map Visualization using DAAM: Diffusion-->
                <!--                    Attentive Attribution Maps-->

            </div>
<!--            Experiment 2 -->


        </div> <!--Section div-->

    </div><!-- End of section main div-->

</div>
</body>
</html>